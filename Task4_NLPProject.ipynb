{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2018 14:25:01 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz from cache at /home/tej/.allennlp/cache/88a963eed085eb23ea78b083dbc304830c27193e439ff1c899290f02e76c0e82.e9f9e059da2ca307f6ee5835c4e2b0a43b1eed2cb8685c5741a0b16249269ff1\n",
      "12/11/2018 14:25:01 - INFO - allennlp.models.archival -   extracting archive file /home/tej/.allennlp/cache/88a963eed085eb23ea78b083dbc304830c27193e439ff1c899290f02e76c0e82.e9f9e059da2ca307f6ee5835c4e2b0a43b1eed2cb8685c5741a0b16249269ff1 to temp dir /tmp/tmplpmg528s\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   vocabulary.type = default\n",
      "12/11/2018 14:25:17 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmplpmg528s/vocabulary.\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'regularizer': [['.*scalar_parameters.*', {'alpha': 0.001, 'type': 'l2'}]], 'encoder': {'hidden_size': 300, 'input_size': 1124, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_input_projection_bias': False}, 'binary_feature_dim': 100, 'type': 'srl', 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'elmo': {'dropout': 0.1, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7ff19b61d5f8>}\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'regularizer': [['.*scalar_parameters.*', {'alpha': 0.001, 'type': 'l2'}]], 'encoder': {'hidden_size': 300, 'input_size': 1124, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_input_projection_bias': False}, 'binary_feature_dim': 100, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'elmo': {'dropout': 0.1, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7ff19b61d5f8>}\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'dropout': 0.1, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7ff19b61d5f8>}\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0.1, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7ff19b61d5f8>}\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.options_file\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmplpmg528s/fta/model.text_field_embedder.elmo.weight_file\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0.1\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "12/11/2018 14:25:17 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "12/11/2018 14:25:17 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 1124, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_input_projection_bias': False} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7ff19b61d5f8>}\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.input_size = 1124\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/11/2018 14:25:45 - INFO - allennlp.common.params -   model.encoder.use_input_projection_bias = False\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.regularizer = [['.*scalar_parameters.*', {'alpha': 0.001, 'type': 'l2'}]]\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.regularizer.list.list.type = l2\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/11/2018 14:25:48 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "12/11/2018 14:25:48 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'token_indexers': {'elmo': {'type': 'elmo_characters'}}, 'type': 'srl'} and extras {}\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {'token_indexers': {'elmo': {'type': 'elmo_characters'}}} and extras {}\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'elmo_characters'} and extras {}\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer from params {} and extras {}\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/11/2018 14:25:49 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ViterbiParser\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from nltk.wsd import lesk\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "    'murder': wordnet.synsets('murder')[0],\n",
    "    'kidnap': wordnet.synsets('kidnap')[0],\n",
    "    'robbed': wordnet.synsets('robbed')[0],\n",
    "    'molest': wordnet.synsets('molest')[0],\n",
    "    'arson': wordnet.synsets('arson')[0],\n",
    "    'fraud': wordnet.synsets('fraud')[2],\n",
    "    'manslaughter': wordnet.synsets('manslaughter')[0],\n",
    "    'abuse': wordnet.synsets('abuse')[2],\n",
    "    'terrorism': wordnet.synsets('terrorism')[0],\n",
    "    'cybercrime': wordnet.synsets('cybercrime')[0],\n",
    "                }\n",
    "mentions = {\n",
    "    'murder':['murder', 'stab', 'homicide', 'shot', 'attacked', 'kill'],\n",
    "    'kidnap':['kidnap'],\n",
    "    'robbed':['robbed', 'theft', 'burglary'],\n",
    "    'molest':['molest', 'rape', 'touched', 'molestation'],\n",
    "    'arson':['arson', 'incendiary'],\n",
    "    'fraud':['fraud'],\n",
    "    'manslaughter':['manslaughter'],\n",
    "    'abuse':['abuse', 'overdose'],\n",
    "    'terrorism':['terror', 'terrorism', 'terrorize'],\n",
    "    'cybercrime':['cybercrime', 'hacker']\n",
    "    \n",
    "}\n",
    "mention_dict = {\n",
    "    'murder': wordnet.synsets('murder')[0],\n",
    "    'kidnap': wordnet.synsets('kidnap')[0],\n",
    "    'robbed': wordnet.synsets('robbed')[0],\n",
    "    'molest': wordnet.synsets('molest')[0],\n",
    "    'arson': wordnet.synsets('arson')[0],\n",
    "    'fraud': wordnet.synsets('fraud')[2],\n",
    "    'manslaughter': wordnet.synsets('manslaughter')[0],\n",
    "    'abuse': wordnet.synsets('abuse')[2],\n",
    "    'terror': wordnet.synsets('terror')[3],\n",
    "    'cybercrime': wordnet.synsets('cybercrime')[0],\n",
    "    'stab': wordnet.synsets('stab')[1],\n",
    "    'homicide': wordnet.synsets('homicide')[0],\n",
    "    'theft': wordnet.synsets('theft')[0],\n",
    "    'burglary': wordnet.synsets('burglary')[0],\n",
    "    'rape': wordnet.synsets('rape')[3],\n",
    "    'incendiary': wordnet.synsets('incendiary')[0],\n",
    "    'overdose': wordnet.synsets('overdose')[0],\n",
    "    'hacker': wordnet.synsets('hacker')[1],\n",
    "    'shot': wordnet.synsets('shot')[0],\n",
    "    'attacked': wordnet.synsets('attacked')[2],\n",
    "    'touched': wordnet.synsets('touched')[0],\n",
    "    'terrorize': wordnet.synsets('terrorize')[0],\n",
    "    'terrorism': wordnet.synsets('terrorism')[0],\n",
    "    'kill': wordnet.synsets('kill')[0],\n",
    "    'molestation': wordnet.synsets('molestation')[0]\n",
    "}\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template, srl_dict):\n",
    "    if(srl_dict == \"\"):\n",
    "        return \"No template matched\"\n",
    "    reverse = False\n",
    "    reverse_set = {'charged', 'accused', 'claimed', 'suspected', 'guilty', 'convicted', 'arrested', 'jailed', 'pleaded', 'sentenced'}\n",
    "    if(srl_dict['verb'] in reverse_set):\n",
    "        reverse = True\n",
    "    role_string = srl_dict['description']\n",
    "    roles = []\n",
    "    for i in range(len(role_string)):\n",
    "        if(role_string[i] == '['):\n",
    "            r = \"\"\n",
    "            i += 1\n",
    "            while i < len(role_string) and role_string[i] != ']':\n",
    "                r += role_string[i]\n",
    "                i += 1\n",
    "            roles.append(r)\n",
    "    if(template == \"murder\"):\n",
    "        suspect = \"\"\n",
    "        victim = \"\"\n",
    "        location = \"\"\n",
    "        time = \"\"\n",
    "        method = \"\"\n",
    "        \n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                victim = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-MNR\" or role[0] == \"ARGM-ADV\" or role[0] == \"ARGM-PRD\"):\n",
    "                method = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, victim = victim, suspect\n",
    "        return \"Murder(\"+suspect+\", \"+victim+\", \"+method+\", \"+location+\", \"+time+\")\"\n",
    "                    \n",
    "    elif(template == \"robbed\"):\n",
    "        suspect = \"\"\n",
    "        item = \"\"\n",
    "        amount = \"\"\n",
    "        time = \"\"\n",
    "        location = \"\"\n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                item = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-PRD\" or role[0] == \"ARGM-ADV\"):\n",
    "                amount = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, item = item, suspect\n",
    "        return \"Robbed(\"+suspect+\", \"+item+\", \"+amount+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"kidnap\"):\n",
    "        suspect = \"\"\n",
    "        hostage = \"\"\n",
    "        ransom = \"\"\n",
    "        time = \"\"\n",
    "        location = \"\"\n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                hostage = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-PRD\" or role[0] == \"ARGM-ADV\"):\n",
    "                ransom = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, hostage = hostage, suspect\n",
    "        return \"Kidnap(\"+suspect+\", \"+hostage+\", \"+ransom+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"molest\"):\n",
    "        suspect = \"\"\n",
    "        victim = \"\"\n",
    "        relation = \"\"\n",
    "        time = \"\"\n",
    "        location = \"\"\n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                victim = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-PRD\" or role[0] == \"ARGM-ADV\"):\n",
    "                relation = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, victim = victim, suspect\n",
    "        return \"Molest(\"+suspect+\", \"+victim+\", \"+relation+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"arson\"):\n",
    "        arsonist = \"\"\n",
    "        prop = \"\"\n",
    "        loss = \"\"\n",
    "        time = \"\"\n",
    "        location = \"\"\n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                arsonist = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                prop = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-PRD\" or role[0] == \"ARGM-ADV\"):\n",
    "                loss = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            arsonist, prop = prop, arsonist\n",
    "        return \"Arson(\"+arsonist+\", \"+prop+\", \"+loss+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"fraud\"):\n",
    "        suspect = \"\"\n",
    "        conspiracy = \"\"\n",
    "        location = \"\"\n",
    "        time = \"\"\n",
    "        \n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                conspiracy = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, conspiracy = conspiracy, suspect\n",
    "        return \"Fraud(\"+suspect+\", \"+conspiracy+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"manslaughter\"):\n",
    "        suspect = \"\"\n",
    "        victim = \"\"\n",
    "        location = \"\"\n",
    "        time = \"\"\n",
    "        method = \"\"\n",
    "        \n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                victim = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-MNR\" or role[0] == \"ARGM-ADV\" or role[0] == \"ARGM-PRD\"):\n",
    "                method = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, victim = victim, suspect\n",
    "        return \"Manslaughter(\"+suspect+\", \"+victim+\", \"+method+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"abuse\"):\n",
    "        suspect = \"\"\n",
    "        drug = \"\"\n",
    "        location = \"\"\n",
    "        time = \"\"\n",
    "        \n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                drug = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, drug = drug, suspect\n",
    "        return \"Abuse(\"+suspect+\", \"+drug+\", \"+location+\", \"+time+\")\"\n",
    "\n",
    "    elif(template == \"terrorism\"):\n",
    "        suspect = \"\"\n",
    "        target = \"\"\n",
    "        method = \"\"\n",
    "        time = \"\"\n",
    "        location = \"\"\n",
    "        \n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                target = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-MNR\" or role[0] == \"ARGM-ADV\" or role[0] == \"ARGM-PRD\"):\n",
    "                method = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, target = target, suspect\n",
    "        return \"Terrorism(\"+suspect+\", \"+target+\", \"+method+\", \"+location+\", \"+time+\")\"\n",
    "    elif(template == \"cybercrime\"):\n",
    "        suspect = \"\"\n",
    "        target = \"\"\n",
    "        method = \"\"\n",
    "        time = \"\"\n",
    "        location = \"\"\n",
    "        \n",
    "        for args in roles:\n",
    "            role = args.split(\":\")\n",
    "            if(role[0] == \"ARG0\"):\n",
    "                suspect = role[1]\n",
    "            elif(role[0] == \"ARG1\"):\n",
    "                target = role[1]\n",
    "            elif(role[0] == \"ARG2\" or role[0] == \"ARGM-MNR\" or role[0] == \"ARGM-ADV\" or role[0] == \"ARGM-PRD\"):\n",
    "                method = role[1]\n",
    "            elif(role[0] == \"ARGM-TMP\"):\n",
    "                time = role[1]\n",
    "            else:\n",
    "                if(role[0] == \"ARGM-LOC\"):\n",
    "                    location = role[1]\n",
    "        if(reverse):\n",
    "            suspect, target = target, suspect\n",
    "        return \"Cybercrime(\"+suspect+\", \"+target+\", \"+method+\", \"+location+\", \"+time+\")\"\n",
    "    else:\n",
    "        return \"No template matched.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_template(sentence):\n",
    "    similarity = 0\n",
    "    best_match = \"\"\n",
    "    for head_word in word_tokenize(sentence):\n",
    "        for best_sense in wordnet.synsets(lmtzr.lemmatize(head_word, 'v')):\n",
    "            if(best_sense is not None):\n",
    "                for temp in template_dict:\n",
    "                    for t in mentions[temp]:\n",
    "                        sim = best_sense.wup_similarity(mention_dict[t])\n",
    "                        if(sim is not None and sim > similarity):\n",
    "                            similarity = sim\n",
    "                            best_match = temp\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(sent):\n",
    "    first_priority = {'killed', 'shooting', 'fired', 'assaulted', 'shot', 'shoots', 'murdered', 'stab','deceiving', 'manslaughter', 'stabbed', 'stabbing', 'raped', 'rape', 'kidnapped', 'kidnap', 'burned', 'stoled', 'robbed', 'rob', 'stole', 'stealed', 'overdosed', 'overdose', 'abuse', 'burnt', 'molested', 'kidnapping'}\n",
    "    second_priority = {'charged', 'accused', 'guilty', 'arrested', 'jailed' , 'suspect', 'sentenced', 'convicted'}\n",
    "    negative_set = {'is', 'was', 'be', 'has', 'am', 'were', 'been', 'have', 'had'}\n",
    "    srl_dict = predictor.predict(\n",
    "      sentence=sent\n",
    "    )\n",
    "    similarity = 0\n",
    "    best_template = get_best_template(sent)\n",
    "    #print(best_template)\n",
    "    srl_tags = \"\"\n",
    "    for verb_dict in srl_dict['verbs']:\n",
    "        verb = verb_dict['verb']\n",
    "        if(verb in negative_set):\n",
    "            continue\n",
    "        if(verb in first_priority):\n",
    "            srl_tags = verb_dict\n",
    "            break\n",
    "        if(verb in second_priority):\n",
    "            srl_tags = verb_dict\n",
    "            similarity += 1.0\n",
    "            continue\n",
    "        for verb_sense in wordnet.synsets(lmtzr.lemmatize(verb_dict['verb'], 'v')):\n",
    "            for mention in mentions[best_template]:\n",
    "                sim = verb_sense.wup_similarity(mention_dict[mention])\n",
    "                if(sim is not None and sim > similarity):\n",
    "                    similarity = sim\n",
    "                    srl_tags = verb_dict\n",
    "    print(srl_tags)\n",
    "    return fill_template(best_template, srl_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Police arrested hackers for breached a womans account in New Delhi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verb': 'account', 'description': 'Hacker breached a [ARG0: womans] [V: account] in New Delhi .', 'tags': ['O', 'O', 'O', 'B-ARG0', 'B-V', 'O', 'O', 'O', 'O']}\n",
      "Cybercrime( womans, , , , )\n"
     ]
    }
   ],
   "source": [
    "a = extract(sentence)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
